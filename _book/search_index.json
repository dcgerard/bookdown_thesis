[
["index.html", "Theory and Methods for Tensor Data Chapter 1 Abstract", " Theory and Methods for Tensor Data David Gerard 2016-05-05 Chapter 1 Abstract We present novel methods and new theory in the statistical analysis of tensor-valued data. A tensor is a multidimensional array. When data come in the form of a tensor, special methods and models are required to capture the dependencies represented by the indexing structure. For such data, it is often reasonable to assume a Kronecker structured covariance model for the random elements within a tensor. A natural type of Kronecker structured covariance model is the array normal model. We develop equivariant and minimax estimators under the array normal model whose risk performances are dramatically better than that of the maximum likelihood estimator. Although we find improved estimators, maximum likelihood estimation is still popular and useful (e.g. for likelihood ratio testing). We study in detail maximum likelihood estimation in separable covariance models, linking it to the relatively modern study of tensor decompositions. This leads us to develop, within this class of Kronecker structured covariance models, likelihood ratio test statistics which are simply represented as the ratio of two scale parameters from two separate tensor decompositions. We then focus our attention on mean estimation for tensor-valued data. We develop new classes of shrinkage estimators that alter the mode-specific singular values from a tensor generalization of the singular value decomposition. These classes often contain tuning parameters, whose selection is difficult. We choose these tuning parameters by minimizing an unbiased estimate of the mean squared error. From simulations, these new estimators outperform matrix-specific estimators when the tensor indexing structure meaningfully represents the heterogeneity of the underlying signal tensor. "],
["intro.html", "Chapter 2 Introduction 2.1 Tensors 2.2 The array normal model 2.3 Contents of chapters", " Chapter 2 Introduction This thesis concerns covariance and mean estimation in tensor-variate data. In this introductory chapter, we will begin by defining important operations over tensors that will be used throughout this thesis. We will then briefly describe the Tucker decomposition. We will follow this by reviewing the array normal model, a statistical model useful in describing the tensor-specific patterns in the data. We will finish this chapter with an outline of the thesis. 2.1 Tensors We define a tensor as a multidimensional array. To be more formal, in the same way that a vector is an element of a vector space, a tensor is an element of a tensor product of vector spaces. In the same way that up to a choice of basis on a real vector space a vector may be represented as a tuple of real numbers, up to a choice of bases on a set of real vector spaces a tensor may be represented as a multidimensional array of real numbers. For this thesis, we will not be concerned with this more formal definition. We will let \\(\\mathbb{R}^{p_1 \\times \\cdots \\times p_K}\\) denote the real vector space of \\(K\\)-order tensors with dimensions \\((p_1,\\ldots,p_K)\\). A tensor \\(\\mathcal{X} \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K}\\) contains elements \\(\\mathcal{X}_{[i_1,\\ldots,i_K]} \\in \\mathbb{R}\\) for \\(i_k = 1,\\ldots,p_k\\) and \\(k = 1,\\ldots,K\\). A \\(1\\)-way tensor (\\(K = 1\\)) is a vector and a \\(2\\)-way tensor (\\(K = 2\\)) is a matrix. Many data sets come in the form of a tensor (beyond \\(K = 1\\) or \\(2\\)). A multivariate longitudinal network data set is a tensor where an element is the value of relation type \\(k\\) from node \\(i\\) to node \\(j\\) at time \\(t\\) (Hoff 2011). A movie can be represented as a tensor where an element of the tensor is the intensity of pixel \\((i,j)\\) at frame \\(t\\). The mean estimates of an ANOVA model may be represented as a tensor where an element of the tensor is the mean value at factor 1 level \\(i\\), factor 2 level \\(j\\), and factor 3 level \\(k\\) (Volfovsky and Hoff 2014) – for example, we might be interested in concentrations of chemical \\(i\\) at location \\(j\\) at time \\(t\\). There are many other fields where tensors naturally arise (Kroonenberg 2008, Kolda and Bader (2009)). In order to analyze tensor data sets, we need tools to manipulate tensors. The first operation we consider is \\(k\\)-mode matricization, or \\(k\\)-mode matrix unfolding. This operation converts a tensor \\(\\mathcal{X} \\in \\mathbb{R}^{p_1\\times\\cdots \\times p_K}\\) into a matrix \\(\\mathcal{X}_{(k)} \\in \\mathbb{R}^{p_k \\times p/p_k}\\) where \\(p = \\prod_{k=1}^Kp_k\\). The rows in the resulting matrix \\(\\mathcal{X}_{(k)}\\) index the \\(k\\)th mode and the columns index all other modes. The formal definition, due to Kolda and Bader (2009), is below: Definition 1 The \\(k\\)-mode matricization of \\(\\mathcal{X} \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K}\\), denoted \\(\\mathcal{X}_{(k)}\\in \\mathbb{R}^{p_k \\times p/p_k}\\), maps element \\((i_1,\\ldots,i_K)\\) in \\(\\mathcal{X}\\) to element \\((i_k,j)\\) in \\(\\mathcal{X}_{(k)}\\) where \\[ j = 1 + \\sum_{\\substack{n = 1 \\\\ n\\neq k}}^{K}(i_n - 1)J_n \\text{ with } J_n = \\prod_{\\substack{ m = 1 \\\\ m \\neq k}}^{n-1}p_m \\] Similarly, we may vectorize a tensor into a vector. Definition 2 The vectorization of \\(\\mathcal{X} \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K}\\), denoted \\(vec\\left(\\mathcal{X}\\right) \\in \\mathbb{R}^{p}\\), maps element \\((i_1,\\ldots,i_K)\\) in \\(\\mathcal{X}\\) to element \\(j\\) in \\(vec\\left(\\mathcal{X}\\right)\\) where \\[ j = 1 + \\sum_{\\substack{k = 1}}^{K}(i_k - 1)J_k \\text{ with } J_k = \\prod_{\\substack{ m = 1}}^{k-1}p_m. \\] As an example of the matricization and vectorization operators, let \\[ \\mathcal{X} = \\left( \\begin{array}{cc|cc} \\mathcal{X}_{[1,1,1]} &amp; \\mathcal{X}_{[1,2,1]} &amp; \\mathcal{X}_{[1,1,2]} &amp; \\mathcal{X}_{[1,2,2]}\\\\ \\mathcal{X}_{[2,1,1]} &amp; \\mathcal{X}_{[2,2,1]} &amp; \\mathcal{X}_{[2,1,2]} &amp; \\mathcal{X}_{[2,2,2]} \\end{array} \\right) \\in \\mathbb{R}^{2 \\times 2 \\times 2}, \\] where the vertical line \\(|\\) denotes the separation of the third indices. We provide the three possible matricizations: \\begin{align*} \\mathcal{X}_{(1)} &amp;= \\left( \\begin{array}{cccc} \\mathcal{X}_{[1,1,1]} &amp; \\mathcal{X}_{[1,2,1]} &amp; \\mathcal{X}_{[1,1,2]} &amp; \\mathcal{X}_{[1,2,2]}\\\\ \\mathcal{X}_{[2,1,1]} &amp; \\mathcal{X}_{[2,2,1]} &amp; \\mathcal{X}_{[2,1,2]} &amp; \\mathcal{X}_{[2,2,2]} \\end{array} \\right),\\\\ \\mathcal{X}_{(2)} &amp;= \\left( \\begin{array}{cccc} \\mathcal{X}_{[1,1,1]} &amp; \\mathcal{X}_{[2,1,1]} &amp; \\mathcal{X}_{[1,1,2]} &amp; \\mathcal{X}_{[2,1,2]}\\\\ \\mathcal{X}_{[1,2,1]} &amp; \\mathcal{X}_{[2,2,1]} &amp; \\mathcal{X}_{[1,2,2]} &amp; \\mathcal{X}_{[2,2,2]} \\end{array} \\right), \\text{ and}\\\\ \\mathcal{X}_{(3)} &amp;= \\left( \\begin{array}{cccc} \\mathcal{X}_{[1,1,1]} &amp; \\mathcal{X}_{[2,1,1]} &amp; \\mathcal{X}_{[1,2,1]} &amp; \\mathcal{X}_{[2,2,1]}\\\\ \\mathcal{X}_{[1,1,2]} &amp; \\mathcal{X}_{[2,1,2]} &amp; \\mathcal{X}_{[1,2,2]} &amp; \\mathcal{X}_{[2,2,2]} \\end{array} \\right). \\end{align*} We also have the resulting vectorization: \\[ vec(\\mathcal{X}) = (\\mathcal{X}_{[1,1,1]},\\mathcal{X}_{[2,1,1]},\\mathcal{X}_{[1,2,1]},\\mathcal{X}_{[2,2,1]},\\mathcal{X}_{[1,1,2]},\\mathcal{X}_{[2,1,2]},\\mathcal{X}_{[1,2,2]},\\mathcal{X}_{[2,2,2]})^T. \\] We will make heavy use the matricization and vectorization operators throughout this thesis. Recall matrix multiplication: For \\(A \\in \\mathbb{R}^{m \\times n}\\) and \\(B \\in \\mathbb{R}^{n \\times p}\\), we have \\(X = AB \\in \\mathbb{R}^{m \\times p}\\) if \\[ X_{[i,j]} = \\sum_{k = 1}^{n}A_{[i,k]}B_{[k,j]}. \\] There are a few types of multiplication between tensors (Bader and Kolda 2004, Kolda (2006), Kilmer and Martin (2011)). For this thesis, we will almost exclusively consider multilinear multiplication, or the Tucker product, between a tensor \\(\\mathcal{A} \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K}\\) and a list of matrices \\(B_k \\in \\mathbb{R}^{q_k \\times p_k}\\) for \\(k = 1,\\ldots,K\\). We have \\(\\mathcal{X} = (B_1,\\ldots,B_K) \\cdot \\mathcal{A} \\in \\mathbb{R}^{q_1\\times\\cdots\\times q_K}\\) if \\begin{equation} \\mathcal{X}_{[j_1,\\ldots,j_K]} = \\sum_{i_1,\\ldots,i_K = 1}^{p_1,\\ldots,p_K}\\mathcal{A}_{[i_1,\\ldots,i_{K}]}B_{1[j_1,i_1]}\\cdots B_{K[j_K,i_K]}. \\label{eq:tuckerdef} \\end{equation} The Tucker product has important properties with regard to the matricization and vectorization operators: \\begin{align} \\mathcal{X} &amp;= (B_1,\\ldots,B_K) \\cdot \\mathcal{A} \\text{ iff}\\\\ \\mathcal{X}_{(k)} &amp;= B_k\\mathcal{A}(B_K^T\\otimes\\cdots\\otimes B_{k+1}^T\\otimes B_{k-1}^T\\otimes\\cdots\\otimes B_1^T) = B_k\\mathcal{A}_{(k)}B_{-k}^T \\text{ iff}\\label{eq:matdef}\\\\ vec(\\mathcal{X}) &amp;= (B_K\\otimes\\cdots\\otimes B_1)vec(\\mathcal{A}), \\label{eq:vecdef} \\end{align} where \\(B^T\\) is the matrix transpose of \\(B\\) and “\\(\\otimes\\)” denotes the Kronecker product. The Kronecker product between two matrices \\(A \\in \\mathbb{R}^{\\ell \\times m}\\) and \\(B\\in \\mathbb{R}^{n \\times p}\\) is the block matrix \\(A \\otimes B \\in \\mathbb{R}^{\\ell n \\times mp}\\) where each block is \\(A_{[i,j]}B\\). That is, \\[ A \\otimes B = \\left( \\begin{array}{cccc} A_{[1,1]}B &amp; A_{[1,2]}B &amp;\\cdots &amp; A_{[1,m]}B\\\\ A_{[2,1]}B &amp; A_{[2,2]}B &amp;\\cdots &amp; A_{[2,m]}B\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ A_{[\\ell,1]}B &amp; A_{[\\ell,2]} &amp; \\cdots &amp; A_{[\\ell,m]}B\\\\ \\end{array} \\right). \\] The Kronecker product has many useful properties: \\begin{align} &amp; \\left(A \\otimes B\\right) \\otimes C = A \\otimes \\left(B \\otimes C\\right) \\nonumber\\\\ &amp; \\left(A \\otimes B\\right)\\left(C \\otimes D\\right) = AC \\otimes BD \\label{eq:kronprop2}\\\\ &amp; \\left(A \\otimes B\\right)^T = \\left(A^T \\otimes B^T\\right)\\nonumber\\\\ &amp; \\left(A \\otimes B\\right)^{-1} = A^{-1}\\otimes B^{-1},\\nonumber \\end{align} where \\(A^{-1}\\) is the inverse of \\(A\\). Using (\\ref{eq:vecdef} and (\\ref{eq:kronprop2}, it is trivial to prove that \\((C_1,\\ldots,C_K)\\cdot[(B_1,\\ldots,B_K)\\cdot \\mathcal{A}] = (C_1B_1,\\ldots,C_KB_K)\\cdot\\mathcal{A}\\). Hence, we will usually allow “\\(\\cdot\\)” to also denote component-wise multiplication between two lists of matrices. The notion of matrix rank extends to tensors in multiple ways. The version that we consider in this thesis is that of . Recall that the rank of a matrix is the dimension of the vector space spanned by its columns and rows. Define the \\(k\\)-mode vectors of a tensor \\(\\mathcal{X} \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K}\\) as the \\(p_k\\) dimensional vectors formed from \\(\\mathcal{X}\\) by varying \\(i_k\\) and keeping the other indices fixed. Then the multilinear rank of the \\(K\\)-order tensor \\(\\mathcal{X}\\) is the the \\(K\\)-tuple, \\((r_1,\\ldots,r_K)\\), where \\(r_k\\) is the dimension of the vector space spanned by the \\(k\\)-mode vectors. Equivalently, \\(r_k\\) is the rank of the \\(k\\)-mode unfolding of \\(\\mathcal{X}\\), \\(\\mathcal{X}_{(k)}\\). The notion of multilinear rank will be most extensively used in Chapter ??. Decomposing a matrix extends to tensors in multiple ways. In the same way that matrix decompositions try to represent patterns in matrices in terms of products of lower dimensional matrices, tensor decompositions seek to find patterns by representing tensors in terms of products of lower dimensional tensors. When a tensor is represented as a Tucker product between a list of matrices and a “core” tensor (\\ref{eq:tuckerdef}), this form of decomposition is called a “Tucker decomposition”. We will not be concerned with the myriad of other tensor decompositions (Kolda and Bader 2009, Kilmer and Martin (2011), Cichocki et al. (2014)). The matrix singular value decomposition (SVD) can be viewed as a Tucker decomposition. Recall that \\(X \\in \\mathbb{R}^{p \\times n}\\) with \\(p \\leq n\\) may be decomposed as the product of an orthogonal matrix \\(U \\in \\mathbb{R}^{p \\times p}\\), a diagonal matrix \\(D =diag(\\sigma_1,\\ldots,\\sigma_p)\\) for \\(\\sigma_1\\geq\\cdots\\geq\\sigma_p\\), and a \\(n \\times p\\) matrix with orthonormal columns \\(V\\). We write the SVD as \\begin{equation} X = UDV^T = (U,V)\\cdot D. \\end{equation} Hence, \\(D\\) is the core tensor and \\(U\\) and \\(V\\) are the component matrices. Since \\(X_{(1)} = X = UDV^T\\) and \\(X_{(2)} = X^T = VDU^T\\) the SVD may be constructed by calculating the left singular vectors of the two matricizations of \\(X\\), followed by deriving the core array from \\(D = U^TXV = (U^T,V^T)\\cdot X\\). A popular method, then, of generalizing the SVD to tensors is to compute the SVD of \\(\\mathcal{X}_{(k)} = U_kD_kV_k^T\\), set \\(\\mathcal{S} = (U_1^T,\\ldots,U_K^T)\\cdot\\mathcal{X}\\), and write: \\[ \\mathcal{X} = (U_1,\\ldots,U_K)\\cdot\\mathcal{S}. \\] This Tucker decomposition is called the higher-order SVD (HOSVD) (L. De Lathauwer, De Moor, and Vandewalle 2000a) and contains many properties which make it seem a natural generalization of the SVD to tensors. It will be considered briefly in Chapter 3 and used extensively in Chapter ??. 2.2 The array normal model In this section, we review the array normal model. We do so by building up from the multivariate normal model. Let \\(X \\in \\mathbb{R}^{p \\times n}\\) such that \\[ X_{[,1]},\\ldots,X_{[,n]} \\overset{i.i.d.}{\\sim} N_p\\left(\\theta,\\Psi_1\\Psi_1^T\\right). \\] This model may be written as \\begin{equation} X \\overset{d}{=} \\theta\\mathbf{1}_n^T + \\Psi_1 Z, \\label{eq:multnormmodelintro} \\end{equation} where \\(Z \\in \\mathbb{R}^{p \\times n}\\) contains standard normal entries. From elementary operations, we have \\[ E[(X - \\theta\\mathbf{1}_n^T)(X - \\theta\\mathbf{1}_n^T)^T] \\propto \\Psi_1\\Psi_1^T. \\] That is, \\(\\Psi_1\\Psi_1^T\\) represents the “row covariance” of \\(X\\). One natural extension of this model is to allow \\(Z\\) in (\\ref{eq:multnormmodelintro}) to be multiplied on the right by another matrix \\(\\Psi_2\\). \\begin{equation} X \\overset{d}{=} \\Theta + \\Psi_1 Z \\Psi_2^T, \\label{eq:matnormmodel} \\end{equation} where \\(Z \\in \\mathbb{R}^{p \\times n}\\) contains standard normal entries. This is called the matrix normal model (Srivastava and Khatri 1979, Dawid (1981)). Under (\\ref{eq:matnormmodel}), it can be shown that \\begin{align} \\begin{split} \\label{eq:expectationmode} E[(X-\\Theta)(X - \\Theta)^T] &amp;\\propto \\Psi_1\\Psi_1^T \\text{ and}\\\\ E[(X-\\Theta)^T(X - \\Theta)] &amp;\\propto \\Psi_2\\Psi_2^T. \\end{split} \\end{align} Intuitively, we may consider \\(\\Psi_1\\Psi_1^T\\) as representing the “row covariance” while \\(\\Psi_2\\Psi_2^T\\) represents the “column covariance”. This model contains \\(p(p+1)/2 + n(n+1)/2 - 1\\) covariance parameters. If we were to have allowed for there to be unrestricted covariance between any element in \\(X\\) and any other element in \\(X\\), then we would have had \\(np(np + 1)/2\\) covariance parameters, which is potentially much larger than the number of covariance parameters in the matrix normal model. Now consider the tensor case \\(\\mathcal{X} \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K}\\). A natural extension of the matrix normal model is to define the covariance structure through the Tucker product. This was done in Hoff (2011): \\[ \\mathcal{X} \\overset{d}{=} \\Theta + (\\Psi_1,\\ldots,\\Psi_K) \\cdot \\mathcal{Z} , \\] where \\(\\Theta \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K}\\) and \\(\\mathcal{Z} \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K}\\) contains standard normal entries. From (\\ref{eq:matdef}) we have \\begin{align*} \\mathcal{X} &amp;\\overset{d}{=} \\Theta + (\\Psi_1,\\ldots,\\Psi_K)\\cdot\\mathcal{Z}\\\\ \\mathcal{X}_{(k)} &amp;\\overset{d}{=} \\Theta_{(k)} + \\Psi_k \\mathcal{Z}_{(k)} \\left ( \\Psi_K^T \\otimes \\cdots \\otimes \\Psi_{k+1}^T \\otimes \\Psi_{k-1}^T \\otimes \\cdots \\otimes \\Psi_1^T \\right )\\\\ &amp;= \\Theta_{(k)} + \\Psi_k \\mathcal{Z}_{(k)} \\Psi_{-k}^T. \\end{align*} From which, using (\\ref{eq:expectationmode}), we can show that \\[ E\\left[(\\mathcal{X}_{(k)} - \\Theta_{(k)})(\\mathcal{X}_{(k)} - \\Theta_{(k)})^T \\right] \\propto \\Psi_k\\Psi_k^T. \\] And thus, we may interpret \\(\\Psi_k\\Psi_k^T\\) as being the covariance among the \\(p_k\\) slices of the array \\(\\mathcal{X}\\) along the \\(k\\)th mode. As well as being a generalization of the multivariate normal model, the array normal model may be viewed as a special case of the multivariate normal model. Using (\\ref{eq:vecdef}), we have \\begin{align*} &amp;\\mathcal{X} \\overset{d}{=} \\Theta + (\\Psi_1,\\ldots,\\Psi_k)\\cdot\\mathcal{Z}\\\\ &amp;\\Leftrightarrow vec(\\mathcal{X}) \\overset{d}{=} vec(\\Theta) + (\\Psi_K \\otimes \\cdots \\otimes \\Psi_1)vec(\\mathcal{Z})\\\\ &amp;\\Leftrightarrow vec(\\mathcal{X}) \\sim N_p(vec(\\Theta),\\Psi_K\\Psi_K^T \\otimes \\cdots \\otimes \\Psi_1\\Psi_1^T). \\end{align*} That is, the array normal model is the multivariate normal model with a Kronecker structured covariance matrix. To summarize, the array normal model is appealing for tensor-variate data sets because of the intuitive interpretation of the mode-specific covariance parameters and because this model is more parsimonious than an unstructured covariance model. That is, the array normal model contains \\(\\frac{1}{2}\\sum_{k=1}^{K}p_k(p_k+1) - K + 1\\) covariance parameters against the \\(\\frac{1}{2}\\prod_{k=1}^Kp_k\\left(\\prod_{k=1}^Kp_k + 1\\right)\\) covariance parameters of the multivariate normal model. The array normal model will be discussed in more detail in Chapters 3 and ??. 2.3 Contents of chapters In Chapter 3, we begin by developing a higher-order generalization of the LQ decomposition. We link this decomposition to its role in likelihood-based estimation and testing for Kronecker structured covariance models. This role is analogous to that of the LQ decomposition in likelihood inference for the multivariate normal model. We then extend the literature on tensor decompositions by showing that this higher-order LQ decomposition can be used to construct an alternative version of the popular higher-order singular value decomposition for tensor-valued data. We then develop a novel generalization of the polar decomposition to tensor-valued data. In Chapter ??, we obtain optimality results for the array normal model that are analogous to some classical results concerning covariance estimation for the multivariate normal model. We show that under a lower triangular product group, a uniformly minimum risk equivariant estimator (UMREE) can be obtained via a generalized Bayes procedure. Although this UMREE is minimax and dominates the MLE, we show that it can be improved upon via an orthogonally equivariant modification. Numerical comparisons of the risks of these estimators show that the equivariant estimators can have substantially lower risks than the MLE. In Chapter ??, we study mean estimation for tensor-variate data. We generalize existing matrix shrinkage methods to the estimation of a tensor of parameters from noisy tensor data. Specifically, we develop new classes of estimators that shrink or threshold the mode-specific singular values from the higher-order singular value decomposition of L. De Lathauwer, De Moor, and Vandewalle (2000a). These classes of estimators are indexed by tuning parameters, which we adaptively choose from the data by minimizing Stein’s unbiased risk estimate. In particular, this procedure provides a way to estimate the multilinear rank of the underlying signal tensor. Using simulation studies under a variety of conditions, we show that our estimators perform well when the mean tensor has approximately low multilinear rank, and perform competitively in the absence of low multilinear rank. We illustrate the use of these methods in an application to multivariate relational data. We conclude this thesis with a discussion and open problems in Chapter ??. In particular, we discuss the existence for the MLE in the array normal model and we discuss minimax estimates of the mean for tensor-variate data. 7 References "],
["chapter-holq.html", "Chapter 3 A Higher-order LQ Decomposition for Separable Covariance Models 3.1 Introduction 3.2 The incredible HOLQ 3.3 The incredible HOLQ for separable covariance inference 3.4 Other tensor decompositions 3.5 Discussion", " Chapter 3 A Higher-order LQ Decomposition for Separable Covariance Models 3.1 Introduction There has been a recent surge of interest in methods for tensor-valued data in the machine learning, applied math, and statistical communities. Tensors, or multiway arrays, are higher-order generalizations of vectors and matrices whose elements are indexed by more than two index sets. Analysis methods for tensor-valued data include tensor decompositions and statistical modeling. The former aims to express the tensor in terms of interpretable lower-dimensional components. The latter uncovers patterns through the lens of statistical inference in a parametric statistical model. The work in the field of tensor decompositions is extensive (see Kolda and Bader (2009) or Cichocki et al. (2014) for a review). A common class of tensor decompositions are Tucker decompositions (Tucker 1966), which, for an array \\(X \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K}\\) with entries \\(X_{[i_1,\\ldots,i_K]}\\), expresses \\(X\\) as a product of a “core” array \\(S \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K}\\) and matrices \\(U_1,\\ldots,U_K\\) where \\(U_k \\in \\mathbb{R}^{p_k\\times p_k}\\), expressed as \\begin{align} \\label{eq:tuckerdecomp} X = (U_1,\\ldots,U_K) \\cdot S, \\end{align} where “\\(\\cdot\\)” is multilinear multiplication defined in Section 2.1, and again later in Section 3.2. Most Tucker decompositions impose orthogonality constraints on the \\(U_k\\)’s. One resulting tensor decomposition with such orthogonality constraints is the higher-order singular value decomposition (HOSVD) of L. De Lathauwer, De Moor, and Vandewalle (2000a) and L. De Lathauwer, De Moor, and Vandewalle (2000b), a generalization of the singular value decomposition (SVD). There are other generalizations of the SVD to tensors outside the Tucker decomposition framework (Silva and Lim 2008, Grasedyck (2010), Kilmer and Martin (2011)). However, our work will focus on Tucker decompositions of the form (\\ref{eq:tuckerdecomp}), where the \\(U_k\\)’s have a variety of forms other than orthogonality. A different perspective on tensor-valued data analysis uses statistical modeling, which aims to capture the dependencies between the entries of a tensor through a parametric model. One such model is the multilinear normal model (Hoff 2011, Ohlson, Rauf Ahmad, and Rosen (2013), Manceur and Dutilleul (2013)) — also known as the “array normal model” or “tensor normal model” — which is an extension of the matrix normal model (Srivastava and Khatri 1979, Dawid (1981)). A \\(p_1 \\times \\cdots \\times p_K\\) tensor \\(X\\) follows a multilinear normal distribution if \\(vec(X)\\) is normally distributed with covariance \\(\\Sigma_K \\otimes \\cdots \\otimes \\Sigma_1\\), where “\\(\\otimes\\)” is the Kronecker product and “\\(vec(\\cdot)\\)” is the vectorization operator. For \\(\\Sigma_k = A_kA_k^T\\), \\(k = 1,\\ldots,K\\), the multilinear normal model may be written \\begin{align} \\label{eq:multnorm} X \\overset{d}{=} (A_1,\\ldots,A_K) \\cdot Z, \\end{align} where \\(Z \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K}\\) contains independent and identically distributed (i.i.d.) standard normal entries. The multilinear normal model “separates” the covariances along the modes, or dimensions of \\(X\\). That is, the dependencies along the \\(k\\)th mode are represented by a single covariance matrix, \\(\\Sigma_k\\). Models where the covariance matrix is Kronecker structured are thus often called “separable covariance models”. Most results for the multilinear normal model can be easily generalized to array-variate elliptically contoured models with separable covariance (Akdemir and Gupta 2011). In Section 3.2, we derive a novel tensor decomposition, a type of Tucker decomposition, whose components provide the maximum likelihood estimators (MLEs) of the parameters in the mean zero multilinear normal model, and array-variate elliptically contoured models with separable covariance in general. This tensor decomposition is a generalization of the LQ matrix decomposition to multiway arrays, and so we call it the incredible Higher-Order LQ decomposition (incredible HOLQ, or just HOLQ). One can view the LQ decomposition as taking the form \\begin{align*} X = \\ell L Q I_n\\in \\mathbb{R}^{p \\times n}, \\end{align*} where \\(\\ell &gt; 0\\), \\(Q\\) has orthonormal rows, \\(L\\) is a lower triangular matrix with positive diagonal elements and unit determinant, and \\(I_n\\) is the identity matrix. The HOLQ takes the form \\begin{align*} X = \\ell(L_1,\\ldots,L_K,I_n)\\cdot Q \\in\\mathbb{R}^{p_1\\times\\cdots\\times p_K \\times n}, \\end{align*} where \\(\\ell &gt; 0\\), each \\(L_k\\) is a lower triangular matrix with positive diagonal elements and unit determinant, and \\(Q \\in \\mathbb{R}^{p_1 \\times \\cdots \\times p_K \\times n}\\) has certain orthogonality properties which generalize the orthonormal rows property of the LQ decomposition. Section 3.3 shows the close relationship between the HOLQ and likelihood inference in the multilinear normal model: In Section 3.3.1, we show that each \\(L_k\\) matrix in the HOLQ is the Cholesky square root of the MLE for the \\(k\\)th component covariance matrix, \\(\\Sigma_k\\), in the multilinear normal model (\\ref{eq:multnorm}). This relationship is analogous to the correspondence between the LQ decomposition and the MLE in the multivariate normal model. In the same way that likelihood estimation in the multilinear normal model is connected to the HOLQ, likelihood inference in submodels of the unconstrained multilinear normal model is connected to other decompositions where the component matrices have certain structures. In Section 3.3.2, we consider constraining \\(\\Sigma_k\\) to be diagonal. This has the interpretation of statistical independence along the \\(k\\)th mode and corresponds to constraining \\(L_k\\) to be diagonal in the related tensor decomposition. We also consider constraining the diagonal of the lower triangular Cholesky square root of \\(\\Sigma_k\\) to be the vector of ones, which relates to a covariance model used in time series analysis. We label as “HOLQ juniors” the class of decompositions that correspond to submodels of the unrestricted mean zero multilinear normal model. In Section 3.3.3, we use HOLQ juniors to develop a class of likelihood ratio tests for covariance models in elliptically contoured random arrays with separable covariance. Other tensor decompositions related to the HOLQ are discussed in Section ??. In Section 3.4.1 we use the HOLQ to create a new higher-order analogue to the SVD where each mode has singular values and vectors separated from the core array. Since this SVD is derived from the incredible HOLQ, we call it the incredible SVD (ISVD). The ISVD may be viewed as a core rotation of the HOSVD. In Section 3.4.2 we use a novel minimization formulation of the polar decomposition to generalize it to tensors. 3.2 The incredible HOLQ Let \\(X \\in \\mathbb{R}^{p \\times n}\\) be of rank \\(p\\) where \\(p \\leq n\\). Recall the LQ decomposition, \\begin{align*} X = LQ, \\end{align*} where \\(L \\in G_{p}^+\\), the set of \\(p\\) by \\(p\\) lower triangular matrices with positive diagonal elements, and \\(Q^T \\in \\mathcal{V}_{p,n}\\), the Stiefel manifold of \\(n\\) by \\(p\\) matrices with orthonormal columns. It is common to formulate the LQ decomposition as a Gram-Schmidt orthogonalization of the rows of \\(X\\). We instead consider an alternative formulation of the LQ decomposition as a minimization problem: Theorem Let \\(\\mathcal{G}_{p}^+\\) denote the set of \\(p\\) by \\(p\\) lower triangular matrices with positive diagonal elements and unit determinant. Let \\begin{align} \\label{eq:lqmin} L = argmin_{\\tilde{L} \\in \\mathcal{G}_{p}^+}\\|\\tilde{L}^{-1}X\\|, \\end{align} where \\(\\|\\cdot\\|\\) is the Frobenius norm. Set \\(\\ell = \\|L^{-1}X\\|\\) and \\(Q = L^{-1}X / \\ell\\). Then \\(X = \\ell LQ\\) is the LQ decomposition of \\(X\\). proof By the uniqueness of the LQ decomposition (Proposition 5.2 of Eaton 1983), it suffices to show that \\(Q\\) has orthonormal rows. We have \\(QQ^T = I_p \\Leftrightarrow L^{-1}XX^TL^{-T}/\\ell^2 = I_p \\Leftrightarrow XX^T = \\ell^2LL^T\\). Also note that the solution in (\\ref{eq:lqmin}) is equivalent to finding the matrix \\(\\tilde{S}\\) that satisfies \\(\\tilde{S} = LL^T = argmin_{S \\in \\mathcal{S}_{p}^{1}}tr(S^{-1}XX^T)\\), where \\(\\mathcal{S}_{p}^{1}\\) is the set of \\(p\\) by \\(p\\) positive definite matrices with unit determinant. If we can show that \\(\\tilde{S} = XX^T / |XX^T|^{1/p}\\) then we have shown that \\(Q\\) has orthonormal rows. Using Lagrange multipliers, we must minimize \\(tr(S^{-1}XX^T) - \\lambda\\log|S|\\) in \\(S \\in \\mathcal{S}_{p}^+\\), the set of \\(p\\) by \\(p\\) positive definite matrices, and \\(\\lambda \\in \\mathbb{R}\\). Equivalently, we could also minimize \\(tr(VXX^T) - \\lambda\\log|V|\\), where \\(V = S^{-1}\\). Temporarily ignoring the symmetry of \\(V\\), taking derivatives (chapter 8 of Magnus and Neudecker 1999) and setting equal to zero we have \\begin{align*} &amp;XX^T - \\lambda V^{-1} = 0 \\text{ and } |V| = 1\\\\ &amp;\\Leftrightarrow \\lambda V^{-1} = XX^T \\text{ and } |V| = 1\\\\ &amp;\\Leftrightarrow V^{-1} = XX^T / |XX^T|^{1/p} \\text{ and } \\lambda = |XX^T|^{1/p}\\\\ &amp;\\Leftrightarrow S = XX^T / |XX^T|^{1/p} \\text{ and } \\lambda = |XX^T|^{1/p}. \\end{align*} Since \\(\\log|V|\\) is strictly concave (Theorem 25 of Chapter 11 of Magnus and Neudecker (1999) or Theorem 7.6.7 of Horn and Johnson (2013)), \\(tr(VXX^T)\\) is linear, and \\(\\lambda = |XX^T|^{1/p} &gt; 0\\), we have that \\(tr(VXX^T) - \\lambda\\log|V|\\) is a convex function in \\(V\\). Hence, \\(S = XX^T / |XX^T|^{1/p}\\) is a global minimum (c.f. Theorem 13 of Chapter 7 in Magnus and Neudecker (1999)). Since \\(XX^T / |XX^T|^{1/p}\\) is symmetric and positive definite, it is also a global minimum over the space of symmetric positive definite matrices. \\(\\Box\\) In (\\ref{eq:lqmin}), we are “dividing out” \\(L\\) from the rows of \\(X\\). In this way, we can consider the formulation of the LQ decomposition in Theorem ?? as finding the \\(L \\in \\mathcal{G}_{p}^+\\) that accounts for the greatest amount of heterogeneity in the rows of \\(X\\). The goal of accounting for the heterogeneity in each mode of a multidimensional array will lead to our generalization of the LQ decomposition to tensors, where \\(X \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K \\times n}\\). Definition If \\begin{align} \\label{eq:arrayopt} (L_1,\\ldots,L_K) = argmin_{\\tilde{L}_k \\in \\mathcal{G}_{p_k}^+,\\ k = 1,\\ldots,K}\\| (\\tilde{L}_1^{-1},\\ldots,\\tilde{L}_K^{-1},I_n) \\cdot X\\| \\end{align} then \\begin{align} \\label{eq:holq} X = \\ell (L_1,\\ldots,L_K,I_n) \\cdot Q \\end{align} is an incredible HOLQ, where \\(\\ell = \\| (L_1^{-1},\\ldots,L_K^{-1},I_n) \\cdot X\\|\\) and \\(Q = (L_1^{-1},\\ldots, L_K^{-1},I_n) \\cdot X / \\ell\\). Here, \\((L_1,\\ldots,L_K,I_n) \\cdot Q\\) denotes of \\(Q\\) by the list of matrices \\((L_1,\\ldots,L_K,I_n)\\) (Silva and Lim 2008), also known as the (Kofidis and Regalia 2001, Hoff (2011)). That is, if \\(X = (L_1,\\ldots,L_K,I_n) \\cdot Q\\) then \\begin{align*} X_{[j_1,\\ldots,j_K,j_{K+1}]} = \\sum_{i_1,\\ldots,i_K = 1}^{p_1,\\ldots,p_K}Q_{[i_1,\\ldots,i_{K},j_{K+1}]}L_{1[j_1,i_1]}\\cdots L_{k[j_K,i_K]}. \\end{align*} Multilinear multiplication has the following useful properties: If (\\ref{eq:holq}) holds, then \\begin{align} &amp;X_{(k)} = L_kQ_{(k)}(I_n \\otimes L_K^T \\otimes \\cdots \\otimes L_{k+1}^T \\otimes L_{k-1}^T \\otimes \\cdots \\otimes L_1^T) \\text{ and} \\label{eq:matricization}\\\\ &amp;vec(X) = (I_n \\otimes L_K \\otimes \\cdots \\otimes L_1)vec(Q), \\label{eq:vectorization} \\end{align} where \\(X_{(k)}\\) is the unfolding of the array \\(X\\) into a \\(p_k\\) by \\(n\\prod_{i \\neq k}^Kp_i\\) matrix and \\(vec(X)\\) is the unfolding of the array \\(X\\) into a \\(n\\prod_{k=1}^Kp_k\\) dimensional vector (Kolda and Bader 2009). We will generally denote \\(I_n \\otimes L_K \\otimes \\cdots \\otimes L_{k+1} \\otimes L_{k-1} \\otimes \\cdots \\otimes L_1\\) by \\(L_{-k}\\) and denote \\(\\prod_{k=1}^Kp_k\\) by \\(p\\). We note that such a minimizing \\((L_1,\\ldots,L_K)\\) in (\\ref{eq:arrayopt}) may not exist. This is discussed further in Section @ref(section:holq_discussion). When such a minimizer does exist, we may use (\\ref{eq:matricization}) and Theorem ?? to develop a block coordinate descent algorithm (Tseng 2001) to solve the minimization problem (\\ref{eq:arrayopt}): At iteration \\(i\\), we fix \\(L_k\\) for \\(k \\neq i\\). We then find the minimizer in \\(L_i \\in \\mathcal{G}_{p_i}^+\\) of \\begin{align*} \\|L_i^{-1}X_{(i)}L_{-i}^{-T}\\|, \\end{align*} which, by Theorem ?? is the \\(L\\) matrix in the LQ decomposition of \\(X_{(i)}L_{-i}^{-T} = \\ell L Q\\). This algorithm is presented in Algorithm ??. A slight improvement on Algorithm ?? is presented in Algorithm ?? where we also update the core array \\(Q\\) of the HOLQ while updating the component lower triangular matrices. Unlike Algorithm ??, Algorithm ?? does not require the calculation of the inverse of \\(L_k\\) or the extra matrix multiplication of \\(X_{(k)}L_{-k}^{-T}\\) at each step. A proof of the equivalence between Algorithms ?? and ?? can be found in Appendix @ref(section:equiv.flip.holq). \\begin{algorithm}[h!] \\caption{Block coordinate descent for the HOLQ.} \\label{algorithm:flipflop} \\begin{algorithmic} \\STATE Given $X \\in \\mathbb{R}^{p_1 \\times \\cdots \\times p_K \\times n}$, initialize: \\STATE $L_k \\leftarrow L_{k0} \\in \\mathcal{G}_{p_k}^+$ for $k = 1,\\ldots,K$. \\STATE $\\ell \\leftarrow \\|(L_{10}^{-1},\\ldots,L_{K0}^{-1},I_n) \\cdot X\\|$ \\REPEAT \\FOR{$k \\in \\{1,\\ldots,K\\}$} \\STATE LQ decomposition of $X_{(k)}L_{-k}^{-T} = LZ^T$ \\STATE $L_k \\leftarrow L / |L|^{1/p_k}$ \\ENDFOR \\UNTIL{Convergence.} \\STATE Set $\\ell \\leftarrow \\|(L_1^{-1},\\ldots,L_K^{-1},I_n)\\cdot X\\|$ \\STATE Set $Q \\leftarrow (L_1^{-1},\\ldots,L_K^{-1},I_n)\\cdot X / \\ell$ \\RETURN $\\ell$, $Q$, and $L_k$ for $k = 1,\\ldots,K$. \\end{algorithmic} \\end{algorithm} \\begin{algorithm}[t!] \\caption{Orthogonalized block coordinate descent for the HOLQ.} \\label{algorithm:holq} \\begin{algorithmic} \\STATE Given $X \\in \\mathbb{R}^{p_1 \\times \\cdots \\times p_K \\times n}$, initialize: \\STATE $L_k \\leftarrow L_{k0} \\in \\mathcal{G}_{p_k}^+$ for $k = 1,\\ldots,K$. \\STATE $\\ell \\leftarrow \\|(L_{10}^{-1},\\ldots,L_{K0}^{-1},I_n) \\cdot X\\|$ \\STATE $Q \\leftarrow (L_{10}^{-1},\\ldots,L_{K0}^{-1},I_n) \\cdot X/\\ell$ \\REPEAT \\FOR{$k \\in \\{1,\\ldots,K\\}$} \\STATE LQ decomposition of $Q_{(k)} = LZ$ \\STATE $Q_{(k)} \\leftarrow Z$ \\STATE $L_k \\leftarrow L_kL$ \\STATE Re-scale: \\begin{description}[noitemsep,nolistsep] \\item $\\ell \\leftarrow \\ell |L_k|^{1/p_k} \\|Q\\|$ \\item $L_k \\leftarrow L_k / |L_k|^{1/p_k}$ \\item $Q \\leftarrow Q/\\|Q\\|$ \\end{description} \\ENDFOR \\UNTIL{Convergence.} \\RETURN $\\ell$, $Q$, and $L_k$ for $k = 1,\\ldots,K$. \\end{algorithmic} \\end{algorithm} There are two things to note about these algorithms. First, at each iteration we are reducing the criterion function \\(\\|(L_1^{-1},\\ldots,L_K^{-1},I_n)\\cdot X\\|\\). Second, at each iteration of Algorithm ??, we are orthonormalizing the rows of the core array, \\(Q\\). Hence, the core array \\(Q\\) of any fixed point of this algorithm, including that of the HOLQ, must have a property which we call : Definition A \\(p_1 \\times \\cdots \\times p_K \\times n\\) tensor \\(Q\\) is if \\begin{align} \\label{eq:scaledorthonormal} Q_{(k)}Q_{(k)}^T = I_{p_k}/p_k \\text{ for all } k = 1,\\ldots,K. \\end{align} Theorem Let \\(X = \\ell (L_1,\\ldots,L_K,I_n) \\cdot Q\\) be an incredible HOLQ. Then the core array \\(Q\\) is scaled all-orthonormal. proof This is a direct consequence of the LQ step in Algorithm ??. \\(\\Box\\) Note that we divide by \\(p_k\\) in (\\ref{eq:scaledorthonormal}) because of the constraint that \\(\\|Q\\| = 1\\). This scaled all-orthonormality property generalizes the orthonormal rows property in the LQ decomposition. Of course, we could have instead generalized the RQ decomposition, where for \\(X \\in \\mathbb{R}^{p\\times n}\\) we have \\(X = RZ\\) for \\(R^T \\in \\mathcal{G}_{p_k}^+\\) and \\(Z^T \\in \\mathcal{V}_{p,n}\\). For \\(X \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K \\times n}\\), if \\(X = \\ell(L_1,\\ldots,L_K,I_n)\\cdot Q\\) is the HOLQ of \\(X\\), we then take the RQ decomposition of each component \\(L_k = R_kZ_k\\), and set \\(r = \\ell\\|(Z_1,\\ldots,Z_K,I_n)\\cdot Q\\|\\) and \\(Z = \\ell(Z_1,\\ldots,Z_K,I_n)\\cdot Q/r\\), then \\(X = r(R_1,\\ldots,R_K,I_n)\\cdot Z\\) is a higher-order RQ (HORQ) of \\(X\\), where \\(Z\\) is scaled all-orthonormal. One could instead have started with a similar minimization formulation of the RQ as we did for the LQ (Theorem ??), then generalize to tensors as we did for the HOLQ (\\ref{eq:holq}), and one would obtain the same HORQ as the one we derive from the HOLQ. 3.3 The incredible HOLQ for separable covariance inference 3.3.1 Maximum likelihood estimation The LQ decomposition of a data matrix has a close relationship to maximum likelihood inference under the multivariate normal model. Assume a data matrix \\(X \\in \\mathbb{R}^{p \\times n}\\) was generated from a \\(N_{p \\times n}(0,I_n \\otimes \\Sigma)\\) distribution for some \\(\\Sigma\\) symmetric and positive definite. That is, the columns of \\(X\\) are assumed to be independently distributed \\(N_p(0,\\Sigma)\\) random vectors. The MLE of \\(\\Sigma\\) is \\(XX^T/n\\), and so is proportional to \\(XX^T = LQQ^TL^T = LL^T\\), where \\(X = LQ\\) is the LQ decomposition of \\(X\\). This result carries over to the multilinear normal model (@ref(eq:mult.norm)) using the HOLQ. Assume the data array \\(X \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K \\times n}\\) follows a multilinear normal model, \\(X \\sim N_{p_1\\times\\cdots\\times p_K \\times n}(0,\\sigma^2I_n\\otimes\\Sigma_K\\otimes\\cdots\\otimes\\Sigma_1)\\). That is, \\begin{align} \\label{eq:multnormmodel} &amp;X \\overset{d}{=} \\sigma(\\Sigma_1^{1/2},\\ldots,\\Sigma_K^{1/2},I_n)\\cdot Z, \\end{align} where \\(Z \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K \\times n}\\) has i.i.d. standard normal entries and \\(\\Sigma_k^{1/2}\\) is the lower triangular Cholesky square root matrix of \\(\\Sigma_k\\) for \\(k = 1,\\ldots,K\\). Here, we use the identifiable parameterization of Gerard and Hoff (2015) where \\(\\Sigma_k \\in \\mathcal{S}_{p_k}^1\\) for \\(k = 1,\\ldots,K\\) and \\(\\sigma^2 &gt; 0\\). The following theorem shows that the MLE of \\((\\sigma^2,\\Sigma_1,\\ldots,\\Sigma_K)\\) can be recovered from the HOLQ of \\(X\\). Theorem Let \\(X = \\ell(L_1,\\ldots,L_K,I_n)\\cdot Q\\) be the incredible HOLQ of \\(X\\). Then under the model (@ref(eq:mult.normmodel)) The MLE of \\(\\Sigma_k\\) is \\(\\hat{\\Sigma}_k = L_kL_k^T\\) for \\(k = 1,\\ldots,K\\), The MLE of \\(\\sigma^2\\) is \\(\\hat{\\sigma}^2 = \\ell^2 / (np)\\), The maximized likelihood is equal to \\begin{align*} \\left(2 \\pi \\hat{\\sigma}^2\\right)^{-np/2}e^{-np/2} = \\left(2 \\pi \\ell^2/(np)\\right)^{-np/2}e^{-np/2}. \\end{align*} proof The log-likelihood is proportional to \\begin{align*} \\frac{-np}{2}\\log\\left(\\sigma^2\\right) - \\frac{1}{2\\sigma^2} \\|(\\Sigma_1^{-1/2},\\ldots,\\Sigma_K^{-1/2},I_n)\\cdot X\\|^2, \\end{align*} where \\(\\Sigma_k^{1/2}\\) is the lower triangular Cholesky square root matrix of \\(\\Sigma_k\\). Holding the \\(\\Sigma_k\\)’s fixed, taking a derivative of \\(\\sigma^2\\) and setting equal to zero, we solve for \\(\\sigma^2\\) and obtain \\(\\hat{\\sigma}^2 = \\|(\\Sigma_1^{-1/2},\\ldots,\\Sigma_K^{-1/2},I_n)\\cdot X\\|^2 / (np)\\). A second derivative test confirms this is the global maximizer for any fixed \\(\\Sigma_1,\\ldots,\\Sigma_K\\). The profiled likelihood is then \\begin{align} \\label{eq:maximizedlike} &amp;\\left(2\\pi\\hat{\\sigma}^2\\right)^{-np/2}\\exp\\left\\{-\\frac{1}{2\\hat{\\sigma}^2} \\|(\\Sigma_1^{-1/2},\\ldots,\\Sigma_K^{-1/2},I_n)\\cdot X\\|^2\\right\\} \\nonumber\\\\ &amp;=\\left(2\\pi\\hat{\\sigma}^2\\right)^{-np/2}\\exp\\left\\{-\\frac{1}{2\\hat{\\sigma}^2} \\hat{\\sigma}^2 np\\right\\}\\nonumber\\\\ &amp;=\\left(2 \\pi \\hat{\\sigma}^2\\right)^{-np/2}e^{-np/2}. \\end{align} Thus, to maximize the likelihood, we must minimize \\(\\hat{\\sigma}^2 = \\frac{1}{np}\\| (\\Sigma_1^{-1/2},\\ldots, \\Sigma_K^{-1/2}, I_n)\\cdot X\\|^2\\) in \\(\\Sigma_k^{1/2} \\in \\mathcal{G}_{p_k}^+\\) for \\(k = 1,\\ldots,K\\). This is the same as the minimization problem solved by the HOLQ in (\\ref{eq:arrayopt}). Hence, the MLE of \\(\\Sigma_k\\) is \\(\\hat{\\Sigma}_k = L_kL_k^T\\). This in turn implies that \\(\\hat{\\sigma}^2 = \\|(\\hat{\\Sigma}_1^{-1/2},\\ldots,\\hat{\\Sigma}_K^{-1/2},I_n)\\cdot X\\|^2 / (np) = \\|(L_1^{-1},\\ldots,L_K^{-1},I_n)\\cdot X\\|^2 / (np) = \\ell^2/(np)\\). We may plug \\(\\hat{\\sigma}^2 = \\ell^2/(np)\\) into (\\ref{eq:maximizedlike}) to obtain the final part of the theorem. \\(\\Box\\) This relationship with the multilinear normal model extends to any array-variate elliptically contoured model with separable covariance. Using our identifiable parameterization, \\(X\\) is a mean zero elliptically contoured random array with separable covariance if its density has the form \\begin{align*} f(x|\\sigma^2,\\Sigma_1,\\ldots,\\Sigma_K) \\propto (\\sigma^2)^{-p/2}g(\\|(\\Sigma_1^{-1/2},\\ldots,\\Sigma_K^{-1/2})\\cdot x\\|^2/\\sigma^2), \\end{align*} for some known \\(g:\\mathbb{R}^+ \\rightarrow \\mathbb{R}^+\\). Using a general result of Anderson, Fang, and Hsu (1986) (see ??), the MLE of \\(\\sigma^2(\\Sigma_K \\otimes \\cdots \\otimes \\Sigma_1)\\) can be shown to be proportional to the MLE under the multilinear normal model. This in turn implies that the MLEs of the component covariance matrices in separable elliptically contoured distributions have the same relationship with the HOLQ as in the multilinear normal model. That is, \\(\\hat{\\Sigma}_k = L_kL_k^T\\) where \\(X = \\ell(L_1,\\ldots,L_K,I_n)\\cdot Q\\). Only the estimation of the scale \\(\\sigma^2\\) might be different, depending on the function \\(g\\). The MLEs of \\(\\sigma^2\\) and the \\(\\Sigma_k\\)’s depend only on \\(\\ell\\) and the \\(L_k\\)’s, not \\(Q\\). This suggests that the core array \\(Q\\) might be ancillary with respect to the covariance parameters \\(\\Sigma_1,\\ldots,\\Sigma_K\\) and \\(\\sigma^2\\), that is, the distribution of \\(Q\\) might not depend on the parameter values. In the next paragraph, we will prove that this is indeed the case, but to do so we first introduce a group of transformations that acts transitively on the parameter space. Consider the group \\begin{align*} \\mathcal{G} = \\{(a,A_1,\\ldots,A_K) : a &gt; 0, A_k \\in \\mathcal{G}_{p_k}^+ \\text{ for } k=1,\\ldots,K\\}, \\end{align*} where the group operation is component-wise multiplication. For example, if \\((a,A_1,\\ldots,A_K)\\), \\((b,B_1,\\ldots,B_K)\\in \\mathcal{G}\\), then we have \\begin{align*} (a,A_1,\\ldots,A_K)(b,B_1,\\ldots,B_K) = (ab,A_1B_1,\\ldots,A_KB_K). \\end{align*} The group acts on the sample space by \\begin{align*} X \\mapsto a(A_1,\\ldots,A_K,I_n)\\cdot X. \\end{align*} The following theorem shows that under this group action, the core array of the HOLQ, if unique, is maximally invariant (uniqueness is discussed briefly in Section @ref(section:holq_discussion)). More generally, this theorem states that the set of core arrays of fixed points from Algorithm ?? is a maximally invariant statistic. In other words, two arrays are in the same orbit of \\(\\mathcal{G}\\) if and only if the set of core arrays of fixed points of Algorithm ?? are the same. Theorem Let \\(X\\) and \\(Y\\) be in \\(\\mathbb{R}^{p_1\\times\\cdots p_K\\times n}\\). Let \\(\\mathcal{Q}_X\\) and \\(\\mathcal{Q}_Y\\) be the set of core arrays from fixed points of Algorithm ?? for \\(X\\) and \\(Y\\), respectively. Then \\(\\mathcal{Q}_X = \\mathcal{Q}_Y\\) if and only if there exist \\(c &gt; 0\\) and \\(C_k \\in \\mathcal{G}_{p_k}^+\\) for \\(k = 1,\\ldots,K\\) such that \\(c(C_1,\\ldots,C_K,I_n)\\cdot X = Y\\). proof We first prove the “only if” part. Assume that \\(\\mathcal{Q}_X = \\mathcal{Q}_Y\\), then we choose one \\(Q\\) in \\(\\mathcal{Q}_X = \\mathcal{Q}_Y\\). Then there exists \\(a,b &gt; 0\\) and \\(A_k,B_k \\in \\mathcal{G}_{p_k}^+\\) for \\(k = 1,\\ldots,K\\) such that \\(X = a(A_1,\\ldots,A_K,I_n)\\cdot Q\\) and \\(Y = b(B_1,\\ldots,B_K,I_n)\\cdot Q\\). One may set \\(c = b/a\\) and \\(C_k = B_kA_k^{-1}\\) to prove that \\(c(C_1,\\ldots,C_K,I_n) \\cdot X = Y\\). We now prove the “if” part. Assume there exist \\(c &gt; 0\\) and \\(C_k \\in \\mathcal{G}_{p_k}^+\\) for \\(k = 1,\\ldots,K\\) such that \\(c(C_1,\\ldots,C_K,I_n)\\cdot X = Y\\). Then for each \\(Q\\) in \\(\\mathcal{Q}_X\\) we have that \\(Y = ca(C_1A_1,\\ldots,C_KA_K,I_n)\\cdot Q\\) for some \\(a &gt; 0\\) and \\(A_k \\in \\mathcal{G}_{p_k}^+\\) for \\(k = 1,\\ldots,K\\). Since fixed points are entirely determined by the scaled all-orthonormality of the core, \\(Q\\) is also in \\(\\mathcal{Q}_Y\\). Likewise any \\(Q\\) in \\(\\mathcal{Q}_Y\\) will also be in \\(\\mathcal{Q}_X\\). Hence \\(\\mathcal{Q}_X = \\mathcal{Q}_Y\\). \\(\\Box\\) By using the above invariance results, we may now prove that \\(\\mathcal{Q}_{X}\\) is ancillary. The group \\(\\mathcal{G}\\) acts on the parameter space by (Hoff 2011) \\begin{align*} \\sigma^2 \\mapsto a^2\\sigma^2 \\text{ and } \\Sigma_k \\mapsto A_k\\Sigma_kA_k^T. \\end{align*} This action is clearly transitive over the parameter space. Hence, the maximally invariant parameter is a constant. Since the distribution of any invariant statistic depends only on the maximally invariant parameter (Theorem 6.3.2 of Lehmann and Romano 2005), the distribution of \\(\\mathcal{Q}_X\\) is ancillary with respect to \\(\\sigma^2\\) and \\(\\Sigma_k\\) for \\(k = 1,\\ldots,K\\). If the MLE is unique, then the core array of the HOLQ is in 1-1 correspondence with \\(\\mathcal{Q}_X\\), and so is also maximally invariant. Hence, the core array from a unique HOLQ is ancillary with respect to the covariance parameters, \\(\\Sigma_1,\\ldots,\\Sigma_K\\), and \\(\\sigma^2\\). This result holds not just for elliptically contoured array-variate models with separable covariance, but also for models of the form \\begin{align} \\label{eq:sepcov} X \\overset{d}{=} \\sigma (\\Sigma_1^{1/2},\\ldots,\\Sigma_K^{1/2},I_n)\\cdot Z, \\end{align} where \\(Z\\) has a fixed distribution such that \\(E[Z] = 0\\), \\(cov(vec(Z)) = I_{np}\\), and \\(\\Sigma_k^{1/2}\\) is the lower triangular Cholesky square root of \\(\\Sigma_k\\). 3.3.2 HOLQ juniors If it is believed that the dependencies along a mode follow a particular pattern, then from the perspective of parameter estimation, it would make sense to fit a structured covariance matrix that corresponds to the pattern along that mode. For example, if it is believed that the “slices” of the array along a particular mode \\(k\\) are statistically independent, then one would use a model with \\(\\Sigma_k\\) restricted to be a diagonal matrix. If the \\(p_k\\) slices along the mode \\(k\\) are believed to be i.i.d., then one could restrict \\(\\Sigma_k\\) to be the identity matrix. If one of the modes \\(k\\) corresponded to data gathered over sequential time points, then one could fit \\(\\Sigma_k\\) to correspond to an auto-regressive covariance model, such as that of containing constant prediction error variances and arbitrary autoregressive coefficients. One could then restrict \\(\\Sigma_k\\) to have its lower triangular Cholesky square root to have unit diagonal (Pourahmadi 1999). Each of these alternatives corresponds to fitting a submodel of an unrestricted separable covariance model. We represent such submodels mathematically as follows: Partition the index set \\(\\left\\{1,\\ldots,K\\right\\}\\) into four non-overlapping sets \\(J_1, J_2, J_3, J_4\\). Let \\(\\mathcal{D}_{p_k}^+\\) denote the group of \\(p_k\\) by \\(p_k\\) positive definite diagonal matrices with unit determinant. Also, let \\(\\mathcal{S}_{p_k}^{Ch}\\) be the space of \\(p_k\\) by \\(p_k\\) symmetric and positive definite matrices whose lower triangular Cholesky square roots have unit diagonal. Assume the model \\(X \\sim N_{p_1\\times\\cdots\\times p_K}(0,\\sigma^2\\Sigma_K\\otimes\\cdots\\otimes\\Sigma_1)\\) where \\(\\Sigma_k\\) is in \\(\\mathcal{S}_{p_k}^1\\), \\(\\mathcal{D}_{p_k}^+\\), \\(\\mathcal{S}_{p_k}^{Ch}\\), or \\(\\{I_{p_k}\\}\\) when \\(k\\) is in \\(J_1\\), \\(J_2\\), \\(J_3\\), or \\(J_4\\), respectively. The collection of sets \\(J_1, J_2, J_3,\\) and \\(J_4\\) corresponds to a submodel where the modes in \\(J_1\\) have unrestricted covariance, the modes in \\(J_2\\) have diagonal covariance, the modes in \\(J_3\\) have constant prediction error variances and arbitrary autoregressive coefficients, and the modes in \\(J_4\\) have independence and homoscedastic covariance structure. If such a submodel represents a close approximation to the truth, then one would expect to obtain better estimates by fitting this submodel than by fitting an unrestricted multilinear normal model. %%Similarly, from the array-decomposition perspective, if it is believed that a simpler structure for a component matrix would explain the heterogeneity in a tensor (in terms of least squares (\\ref{eq:arrayopt})) approximately as well as a more complex structure, then it would make sense to constrain the component matrix for that mode according to the simpler structure. Some structures to consider for each \\(L_k\\) in (\\ref{eq:holq}) is for \\(L_k\\) to be in \\(\\mathcal{G}_{p_k}^+\\), \\(\\mathcal{D}_{p_k}^+\\), \\(\\mathcal{G}_{p_k}^{Ch}\\), or \\(\\{I_{p_k}\\}\\), where \\(\\mathcal{G}_{p_k}^{Ch}\\) denotes the set of \\(p_k\\) by \\(p_k\\) lower triangular matrices with unit diagonal. We call a decomposition where we constrain some component matrices to be of a particular structure to be a HOLQ junior. In the same way that the HOLQ provides the MLEs in the multilinear normal model, the MLEs in submodels of the unconstrained multilinear normal model are provided by a class of Tucker decompositions we call HOLQ juniors. A HOLQ junior is found by constraining the component matrices in the Tucker decomposition to be in a subspace of \\(\\mathcal{G}_{p_k}^+\\). In particular, we consider constraining each \\(L_k\\) in (\\ref{eq:holq}) to be in \\(\\mathcal{G}_{p_k}^+\\), \\(\\mathcal{D}_{p_k}^+\\), \\(\\mathcal{G}_{p_k}^{Ch}\\), or \\(\\{I_{p_k}\\}\\), where \\(\\mathcal{G}_{p_k}^{Ch}\\) denotes the set of \\(p_k\\) by \\(p_k\\) lower triangular matrices with unit diagonal. Definition[HOLQ junior] Let \\(\\mathcal{G}^{(k)} = \\mathcal{G}_{p_k}^+\\), \\(\\mathcal{D}_{p_k}^+\\), \\(\\mathcal{G}_{p_k}^{Ch}\\), or \\(\\{I_{p_k}\\}\\) if \\(k\\) is in \\(J_1\\), \\(J_2\\), \\(J_3\\), or \\(J_4\\), respectively. If \\begin{align*} (L_1,\\ldots,L_K) = argmin_{\\tilde{L}_k \\in \\mathcal{G}^{(k)},\\ k=1,\\ldots,K}\\| (\\tilde{L}_1^{-1},\\ldots,\\tilde{L}_K^{-1}) \\cdot X\\|, \\end{align*} then \\begin{align} \\label{eq:holqjunior} X = \\ell (L_1,\\ldots,L_K) \\cdot Q \\end{align} is a HOLQ junior, where \\(\\ell = \\| (L_1^{-1},\\ldots,L_K^{-1}) \\cdot X\\|\\) and \\(Q = (L_1^{-1},\\ldots,L_K^{-1}) \\cdot X / \\ell\\). The core array of a HOLQ junior also has a special structure that we prove in the following theorem. Theorem Let \\(X = \\ell (L_1,\\ldots,L_K) \\cdot Q\\) be a HOLQ junior (\\ref{eq:holqjunior}). Then the core array has the following properties: \\(Q_{(k)}Q_{(k)}^T = I_{p_k}/p_k\\) for all \\(k \\in J_1\\), \\(diag\\left(Q_{(k)}Q_{(k)}^T\\right) = \\mathbf{1}_{p_k}/p_k\\) for all \\(k \\in J_2\\), where \\(\\mathbf{1}_{p_k} \\in \\mathbb{R}^{p_k}\\) is the vector of \\(1\\)’s, and \\(Q_{(k)}Q_{(k)}^T = D_k\\) for some diagonal matrix \\(D_k\\) for all \\(k \\in J_3\\). proof We may update the modes for which \\(k \\in J_1\\) using Theorem ?? the same way we did in Algorithm ??. The core array of any fixed point must then have the property that \\(Q_{(k)}Q_{(k)}^T = I_{p_k}/p_k\\) for all \\(k \\in J_1\\). The proofs for \\(k \\in J_2\\) and \\(k \\in J_3\\) follow along the same lines as in the proof for \\(k \\in J_1\\), and are in Appendices ?? and ??. \\(\\Box\\) %%In particular, in the square matrix case where \\(K = 2\\), \\(p_1 = p_2\\), and \\(J_3 = \\{1,2\\}\\) (i.e. \\(L_k \\in \\mathcal{G}_{p_k}^{Ch}\\) for \\(k = 1,2\\)), we have that \\(X = L_1QL_2^T\\), the LDU decomposition, where the all-orthogonal core \\(Q\\) is a diagonal matrix. We can thus consider the HOLQ junior as not only a generalization of the LQ decomposition for tensors, but also of the LDU decomposition. The same arguments as used in Section 3.3.1 show that maximum likelihood inference in multilinear normal submodels has a close connection with HOLQ juniors. The proof of the following is very similar to that of Theorem ?? and is omitted. Theorem Let \\(X = \\ell(L_1,\\ldots,L_K)\\cdot Q\\) be a HOLQ junior. We assume the model \\(X \\sim N_{p_1\\times\\cdots\\times p_K}(0,\\sigma^2\\Sigma_K\\otimes\\cdots\\otimes\\Sigma_1)\\) where \\(\\Sigma_k\\) is in \\(\\mathcal{S}_{p_k}^1\\), \\(\\mathcal{D}_{p_k}\\), \\(\\mathcal{S}_{p_k}^{Ch}\\), or \\(\\{I_{p_k}\\}\\) when \\(k\\) is in \\(J_1\\), \\(J_2\\), \\(J_3\\), or \\(J_4\\), respectively. We have the following: The MLE of \\(\\Sigma_k\\) is \\(L_kL_k^T\\) for \\(k = 1,\\ldots,K\\), The MLE of \\(\\sigma^2\\) is \\(\\ell^2 / (np)\\), The maximum of the likelihood is equal to \\begin{align*} \\left(2 \\pi \\hat{\\sigma}^2\\right)^{-np/2}e^{-np/2} = \\left(2 \\pi \\ell^2/(np)\\right)^{-np/2}e^{-np/2}. \\end{align*} We note here that the same group invariance arguments as used in Section 3.3.1 prove that the core array from a unique HOLQ junior is ancillary with respect to the covariance parameters in separable covariance models. That is, a core array from a unique HOLQ junior (\\ref{eq:holqjunior}) is ancillary under the model \\begin{align} \\label{eq:sepcov} X = \\sigma (\\Sigma_1^{1/2},\\ldots,\\Sigma_K^{1/2})\\cdot Z, \\end{align} where \\(Z\\) has a fixed distribution such that \\(E[Z] = 0\\), \\(cov(vec(Z)) = I_{p}\\), and \\(\\Sigma_k^{1/2}\\) is the lower Cholesky square root of \\(\\Sigma_k\\) in \\(\\mathcal{S}_{p_k}^1\\), \\(\\mathcal{D}_{p_k}^+\\), \\(\\mathcal{S}_{p_k}^{Ch}\\), or \\(\\{I_{p_k}\\}\\) when \\(k\\) is in \\(J_1\\), \\(J_2\\), \\(J_3\\), or \\(J_4\\), respectively. Equivalently, \\(\\Sigma_k^{1/2}\\) is in \\(\\mathcal{G}_{p_k}^+\\), \\(\\mathcal{D}_{p_k}^+\\), \\(\\mathcal{G}_{p_k}^{Ch}\\), or \\(\\{I_{p_k}\\}\\) when \\(k\\) is in \\(J_1\\), \\(J_2\\), \\(J_3\\), or \\(J_4\\), respectively 3.3.3 Likelihood ratio testing One would expect to lose efficiency in covariance estimation when fitting a large model when a submodel is a close approximation to the truth. To aid modeling decisions, we develop a class of likelihood ratio tests (LRTs) for comparing nested separable models. For example, a test of independence across slices of mode \\(k\\) would correspond to \\(H_0: \\Sigma_k \\in \\mathcal{D}_{p_k}^+\\) versus \\(H_1: \\Sigma_k \\in \\mathcal{S}_{p_k}^1\\). A test for independence and heteroscedasticity against independence and homoscedasticity along mode \\(k\\) would correspond to \\(H_0: \\Sigma_k = I_{p_k}\\) versus \\(H_1: \\Sigma_k \\in \\mathcal{D}_{p_k}^+\\). In a longitudinal setting, testing for the presence of non-zero autoregressive coefficients along mode \\(k\\) would correspond to \\(H_0: \\Sigma_k = I_{p_k}\\) versus \\(H_1: \\Sigma_k \\in \\mathcal{S}_{p_k}^{Ch}\\). As seen in Section 3.3.2, each submodel of the unstructured multilinear normal model corresponds to a HOLQ junior. If we have two models \\(H_0\\) and \\(H_1\\), with \\(H_0\\) nested in \\(H_1\\), then the likelihood ratio test takes on the simple form of the ratio of the two scale estimates of the HOLQ juniors corresponding to \\(H_0\\) and \\(H_1\\). Theorem Suppose \\(H_0\\) is a submodel of \\(H_1\\). Suppose \\(vec(X) = \\ell(L_K\\otimes\\cdots\\otimes L_1)vec(Q)\\) and \\(vec(X) = a(A_M\\otimes\\cdots\\otimes A_1)vec(Z)\\) are two HOLQ juniors in vectorized form (\\ref{eq:vectorization}) corresponding to \\(H_0\\) and \\(H_1\\), respectively. Hence, \\(\\hat{\\sigma}_0^2 = \\ell^2/p\\) and \\(\\hat{\\sigma}_1^2 = a^2/p\\) are the MLEs of the scale parameters under \\(H_0\\) and \\(H_1\\), respectively. Then the LRT of \\(H_0\\) versus \\(H_1\\) rejects for large values of \\(\\hat{\\sigma}_0^2/\\hat{\\sigma}_1^2\\), or equivalently \\(\\ell / a\\). proof Applying Theorem 1 from Anderson, Fang, and Hsu (1986) and Theorem ?? (see ??), the LRT rejects for large values of \\begin{align*} \\hat{\\sigma}_1^{-p} / \\hat{\\sigma}_0^{-p} = a^{-p}/\\ell^{-p} = \\ell^p/a^p, \\end{align*} or, equivalently, for large values of \\(\\ell / a\\). \\(\\Box\\) The LRT in Theorem ?? includes testing for a Kronecker structured covariance matrix along modes \\(k\\) and \\(j\\) against an unrestricted covariance matrix along the concatenated modes of \\(k\\) and \\(j\\). That is, it allows for the test \\(H_0: \\Sigma_{kj} = \\Sigma_k \\otimes \\Sigma_j\\) for \\(\\Sigma_k \\in \\mathcal{S}_{p_k}^1\\) and \\(\\Sigma_j \\in \\mathcal{S}_{p_j}^1\\) versus \\(H_1: \\Sigma_{ij} \\in \\mathcal{S}_{p_kp_j}^1\\). This is why \\(M\\) may be different from \\(K\\). For example, if all modes in \\(H_0\\) and \\(H_1\\) had the same covariance structure except modes \\(k\\) and \\(j\\), for which \\(H_0\\) assumes has separable covariance and for which \\(H_1\\) assumes has unstructured covariance along the concatenated mode \\(kj\\), then \\(M = K-1\\). This particular type of test is useful for determining how much separability is reasonable to assume in a covariance matrix. The likelihood ratio test has a nice intuitive interpretation. Since the MLE of \\(\\sigma^2\\) under \\(H_0\\) is \\(\\hat{\\sigma}_0^2 = \\ell^2/p = \\|(L_1^{-1},\\ldots,L_K^{-1})\\cdot X\\|^2/p\\) (Theorem ??), one can consider \\(\\hat{\\sigma}_0^2\\) as a sort of mean squares left after accounting for covariance/heterogeneity along modes \\(1,\\ldots,K\\). Likewise \\(\\hat{\\sigma}_1^2\\) is a sort of mean squares left after accounting for covariance/heterogeneity along modes \\(1,\\ldots,M\\). The likelihood ratio test rejects the null when we can explain significantly more heterogeneity in \\(X\\) by increasing the complexity of the covariance structure. For many hypothesis tests, the distribution of \\(p\\left(\\log\\left(\\ell^2\\right) - \\log\\left(a^2\\right)\\right)\\), the log-likelihood ratio statistic, can be approximated by a \\(\\chi^2\\) distribution. However, this asymptotic approximation would be suspect for small sample sizes. We propose using a Monte Carlo approximation to the null distribution of the LRT statistic. This Monte Carlo approximation can be made arbitrarily precise. The following theorem, whose proof is in Appendix ??, suggests how to sample from the null distribution of the LRT statistic, \\(\\ell / a\\), or \\(\\hat{\\sigma}_0/\\hat{\\sigma}_1\\), in Theorem ??. Theorem Under \\(H_0\\), the distribution of \\(\\ell/a\\) in Theorem ?? does not depend on the parameter values \\(\\Sigma_1,\\ldots,\\Sigma_K,\\) and \\(\\sigma^2\\). This property of the LRT statistic was noted by Lu and Zimmerman (2005) for the matrix-normal case. An immediate implication of Theorem ?? is that for tests of these covariance models, a Monte Carlo sample of the LRT statistic under \\(H_0\\) can be made by simulating values of \\(\\ell/a\\) under \\(H_0\\). A single value of \\(\\ell/a\\) may be simulated from \\(H_0\\) as follows: sample \\(x \\sim N_{p}\\left(0,I_{p}\\right)\\), construct \\(X_1 \\in \\mathbb{R}^{p_1 \\times \\cdots \\times p_K}\\) and \\(X_2 \\in \\mathbb{R}^{q_1\\times \\cdots \\times q_M}\\) from \\(x\\), calculate HOLQ juniors \\(X_1 = \\ell(L_1,\\ldots,L_K)\\cdot Q\\) and \\(X_2 = a(A_1,\\ldots, A_M)\\cdot Z\\), calculate \\(\\ell/a\\). 3.4 Other tensor decompositions 3.4.1 The incredible SVD The incredible HOLQ (\\ref{eq:holq}) may be used to derive a higher-order analogue to the SVD that is related to the HOSVD of L. De Lathauwer, De Moor, and Vandewalle (2000a) and L. De Lathauwer, De Moor, and Vandewalle (2000b). From (\\ref{eq:holq}), we take the SVD of each component lower triangular matrix, \\(L_k = U_kD_kV_k^T\\) for \\(k = 1,\\ldots,K\\). Letting \\(V = (V_1^T,\\ldots,V_K^T,I_n) \\cdot Q\\), we now have an exact decomposition of the data array \\(X\\) which may be viewed as a higher-order generalization of the SVD. Definition Suppose \\begin{align} \\label{eq:ISVD} X = \\ell (U_1,\\ldots,U_K,I_n) \\cdot [(D_1,\\ldots,D_K,I_n) \\cdot V] \\end{align} such that \\(\\ell \\geq 0\\), \\(U_k \\in \\mathcal{O}_{p_k}\\), the set of \\(p_k\\) by \\(p_k\\) orthogonal matrices, for all \\(k = 1,\\ldots,K\\), \\(D_k \\in \\mathcal{D}_{p_k}^+\\), for all \\(k = 1,\\ldots,K\\), and \\(V\\) is scaled all-orthonormal. Then we say that (\\ref{eq:ISVD}) is an incredible SVD (ISVD). The ISVD can be seen as a type of “core rotation” (Kolda and Bader 2009) of the HOSVD. The core is rotated to a form where we may separate the “mode specific singular values”, \\(D_1,\\ldots,D_K\\), from the core. Where the core array in the HOSVD is all-orthogonal (the mode-\\(k\\) unfolding contains orthogonal, but not necessarily orthonormal, rows for all \\(k = 1,\\ldots,K\\)), the core array in the ISVD is scaled all-orthonormal. A low rank version of the ISVD can be defined by finding, for \\(r_k \\leq p_k\\) for \\(k = 1,\\ldots,K\\), the \\(U_k \\in \\mathcal{V}_{r_k,p_k}\\), \\(D_k \\in \\mathcal{D}_{r_k}^+\\) for \\(k = 1,\\ldots,K\\), \\(\\ell &gt; 0\\), and \\(V \\in \\mathbb{R}^{r_1\\times\\cdots\\times r_K \\times n}\\) that minimize \\begin{align} \\label{eq:fnorm} \\|X - \\ell (U_1,\\ldots,U_K,I_n) \\cdot [(D_1,\\ldots,D_K,I_n) \\cdot V]\\|^2. \\end{align} We can apply the HOOI (higher-order orthogonal iteration of L. De Lathauwer, De Moor, and Vandewalle 2000b) to obtain the minimizer of (\\ref{eq:fnorm}). Let \\(X = (V_1,\\ldots,V_K,I_n) \\cdot S\\) be the HOOI of \\(X\\). This minimizes \\begin{align*} \\|X - (V_1,\\ldots,V_K,I_n) \\cdot S\\|^2, \\end{align*} for arbitrary core array \\(S \\in \\mathbb{R}^{r_1\\times\\cdots\\times r_K,n}\\) and arbitrary \\(V_k \\in \\mathcal{V}_{r_k,p_k}\\). We now take the ISVD of \\(S = \\ell (W_1,\\ldots,W_K,I_n) \\cdot [(D_1,\\ldots,D_K,I_n) \\cdot V]\\). We set \\(U_k = V_kW_k\\) for \\(k = 1,\\ldots,K\\). These values now minimize (\\ref{eq:fnorm}). The truncated ISVD does not improve the fit of the low rank array to the data array over the HOOI. Rather, the truncated ISVD can be seen as a core rotation of the HOOI, the same as how the ISVD can be seen as a core rotation of the HOSVD. Again, the core is rotated to a form where we may separate the mode specific singular values, \\(D_1,\\ldots,D_K\\), from the core. 3.4.2 The IHOP decomposition In this section, we explore how our minimization approach may lead to another novel Tucker decomposition. Let \\(X\\) be a \\(p\\) by \\(n\\) matrix with \\(p \\leq n\\) such that \\(X\\) is of rank \\(p\\). We may write \\(X\\) as \\begin{align*} X = PW, \\end{align*} where \\(P \\in \\mathcal{S}_{p}^+\\) and \\(W^T \\in \\mathcal{V}_{p,n}\\). This is known as the (left) (see, for example, Proposition 5.5 of Eaton (1983)). Following the theme of this chapter, we reformulate the polar decomposition as a minimization problem. Let \\(\\mathcal{S}_{p}^F\\) denote the space of \\(p\\) by \\(p\\) positive definite matrices with unit trace. Theorem Let \\begin{align} \\label{eq:polarmin} P = argmin_{\\tilde{P} \\in \\mathcal{S}_{p}^F} tr(\\tilde{P}^{-1}XX^T). \\end{align} Set \\(\\ell = \\|P^{-1}X\\|\\) and \\(W = P^{-1}X/\\ell\\). Then \\begin{align*} X = \\ell P W \\end{align*} is the polar decomposition of \\(X\\). proof By the uniqueness of the polar decomposition (Proposition 5.5 of Eaton 1983), it suffices to show that \\(W\\) has orthonormal rows. We have that \\(WW^T = I_{p} \\Leftrightarrow P^{-1}XX^TP^{-1}/\\ell^2 = I_p \\Leftrightarrow XX^T = \\ell^2PP\\). Hence, if we can show that \\(PP \\propto XX^T\\) then we have shown that \\(W\\) has orthonormal rows. Using Lagrange multipliers, we must minimize \\(tr(P^{-1}XX^T) + \\lambda(tr(P) - 1)\\). This is equivalent to minimizing \\(tr(VXX^T) + \\lambda(tr(V^{-1}) - 1)\\) where \\(V = P^{-1}\\). Temporarily ignoring the symmetry, taking derivatives, and setting equal to \\(0\\), we have \\begin{align*} &amp;XX^T - \\lambda V^{-1}V^{-1} = 0 \\text{ and } tr(V^{-1}) = 1\\\\ &amp;\\Leftrightarrow XX^T = \\lambda V^{-1}V^{-1} \\text{ and } tr(V^{-1}) = 1\\\\ &amp;\\Rightarrow V^{-1} = (XX^T)^{1/2} / tr((XX^T)^{1/2}) \\text{ and } \\lambda = tr((XX^T)^{1/2})^2\\\\ &amp;\\Rightarrow P = (XX^T)^{1/2} / tr((XX^T)^{1/2}) \\text{ and } \\lambda = tr((XX^T)^{1/2})^2, \\end{align*} where \\((XX^T)^{1/2}\\) is any square root matrix of \\(XX^T\\). Let \\((XX^T)^{1/2}\\) now be the unique symmetric square root matrix of \\(XX^T\\), which is a critical point of \\(tr(VXX^T) + \\lambda(tr(V^{-1}) - 1)\\) over the space of positive definite matrices. From problem 2 of Section 7.6 in Horn and Johnson (2013), we have that \\(tr(V^{-1})\\) is strictly convex on the set of positive definite matrices. Since \\(\\lambda = tr((XX^T)^{1/2})^2 &gt; 0\\), we have that \\(tr(VXX^T) + \\lambda(tr(V^{-1}) - 1)\\) is a convex function for all positive definite \\(V\\). Therefore \\(P = (XX^T)^{1/2} / tr((XX^T)^{1/2})\\) is a global minimum (c.f. Theorem 13 of Chapter 7 in Magnus and Neudecker (1999)). \\(\\Box\\) For \\(X \\in \\mathbb{R}^{p_1\\times\\cdots\\times p_K \\times n}\\), we now define the incredible higher-order polar decomposition (IHOP). Definition If \\begin{align} \\label{eq:ihopmin} (P_1,\\ldots,P_K) = argmin_{P_k\\in\\mathcal{S}_{p_k}^F, k = 1,\\ldots,K} tr[(P_K^{-1}\\otimes\\cdots\\otimes P_1^{-1})X_{(K+1)}^TX_{(K+1)}], \\end{align} then \\begin{align*} X = \\ell(P_1,\\ldots,P_K,I_n)\\cdot W \\end{align*} is an IHOP, where \\(\\ell = \\|(P_1^{-1},\\ldots,P_K^{-1},I_n)\\cdot X\\|\\) and \\(W = (P_1^{-1},\\ldots,P_K^{-1},I_n)\\cdot X / \\ell\\). Let \\(\\mathcal{G}_{p}^F\\) be the space of lower triangular matrices with positive diagonal elements and unit Frobenius norm. To derive a block coordinate descent algorithm to find the solution to (\\ref{eq:ihopmin}), we note that (\\ref{eq:polarmin}) is equivalent to finding the \\(L \\in \\mathcal{G}_{p}^F\\) such that \\begin{align*} L = argmin_{\\tilde{L}\\in\\mathcal{G}_{p}^F}\\|\\tilde{L}^{-1}X\\|, \\end{align*} and then setting \\(P = LL^T\\) for \\(P\\) from (\\ref{eq:polarmin}). Hence, (\\ref{eq:ihopmin}) is equivalent to finding \\(L_k \\in \\mathcal{G}_{p_k}^F\\) for \\(k = 1,\\ldots,K\\) such that \\begin{align} (L_1,\\ldots,L_K) = argmin_{\\tilde{L}_k\\in\\mathcal{G}_{p_k}^F, k = 1,\\ldots,K}\\|(\\tilde{L}_1^{-1},\\ldots,\\tilde{L}_K^{-1},I_n)\\cdot X\\|, \\end{align} then setting \\(P_k = L_kL_k^T\\) for \\(k = 1,\\ldots,K\\). At iteration \\(i\\), fix \\(L_k\\) for \\(k \\neq i\\). We then find the minimizer in \\(L_i \\in \\mathcal{G}_{p_i}^F\\) of \\begin{align*} \\|L_i^{-1}X_{(i)}L_{-i}^{-T}\\| = tr(P_i^{-1}X_{(i)}P_{-i}^{-1}X_{(i)}^T), \\end{align*} which, by Theorem ?? is \\(L \\in \\mathcal{G}_{p_k}^F\\) such that \\(LL^TW = X_{(i)}L_{-i}^{-1}\\) is the polar decomposition of \\(X_{(i)}L_{-i}^{-1}\\). This algorithm is presented in Algorithm ??. Again following the theme in this chapter, we present a slightly improved algorithm in Algorithm ??. A proof that Algorithm ?? and Algorithm ?? are equivalent can be found in Appendix ??. From the Algorithm ??, we see that any fixed point of \\(R\\) in Algorithm ?? must have the property that \\(R_{(k)} = L_kZ\\) for the current value of \\(L_k\\) and some \\(Z\\) with orthonormal rows. % The IHOP may be found from the ISVD by setting \\(P_k = U_kD_kU_k^T/tr(U_kD_kU_k^T)\\) for \\(k = 1,\\ldots,K\\), \\(W = (U_1,\\ldots,U_K,I_n)\\cdot V\\), and adjusting the scale appropriately. \\begin{algorithm}[h!] \\caption{Block coordinate descent for the IHOP.} \\label{algorithm:ihop} \\begin{algorithmic} \\STATE Given $X \\in \\mathbb{R}^{p_1 \\times \\cdots \\times p_K \\times n}$, initialize: \\STATE $L_k \\leftarrow L_{k0} \\in \\mathcal{G}_{p_k}^F$ for $k = 1,\\ldots,K$. \\REPEAT \\FOR{$k \\in \\{1,\\ldots,K\\}$} \\STATE Polar decomposition of $X_{(k)}L_{-k}^{-1} = PZ^T$ \\STATE Cholesky decomposition of $P = LL^T$ \\STATE $L_k \\leftarrow L / \\|L\\|$ \\ENDFOR \\UNTIL{Convergence.} \\STATE Set $P_k \\leftarrow L_kL_k^T$ for $k = 1,\\ldots,K$ \\STATE Set $\\ell \\leftarrow \\|(P_1^{-1},\\ldots,P_K^{-1},I_n)\\cdot X\\|$ \\STATE Set $W \\leftarrow (P_1^{-1},\\ldots,P_K^{-1},I_n)\\cdot X / \\ell$ \\RETURN $\\ell$, $W$, and $P_k$ for $k = 1,\\ldots,K$. \\end{algorithmic} \\end{algorithm} \\begin{algorithm}[ht!] \\caption{Orthogonalized block coordinate descent for the IHOP.} \\label{algorithm:pancake} \\begin{algorithmic} \\STATE Given $X \\in \\mathbb{R}^{p_1 \\times \\cdots \\times p_K \\times n}$, initialize: \\STATE $L_k \\leftarrow L_{k0} \\in \\mathcal{G}_{p_k}^F$ for $k = 1,\\ldots,K$. \\STATE $\\ell \\leftarrow \\|(L_1^{-1},\\ldots,L_K^{-1},I_n)\\cdot X\\|$ \\STATE $R \\leftarrow (L_1^{-1},\\ldots,L_K^{-1},I_n)\\cdot X / \\ell$ \\REPEAT \\FOR{$k \\in \\{1,\\ldots,K\\}$} \\STATE Polar decomposition of $L_kR_{(k)} = PZ$ \\STATE Cholesky decomposition of $P = LL^T$ \\STATE Set $R_{(k)} \\leftarrow L^TZ$ \\STATE Set $L_k \\leftarrow L$ \\STATE Re-scale: \\begin{description}[noitemsep,nolistsep] \\item $\\ell \\leftarrow \\ell \\|L_k\\| \\|R\\|$ \\item $L_k \\leftarrow L_k / \\|L_k\\|$ \\item $R \\leftarrow R/\\|R\\|$ \\end{description} \\ENDFOR \\UNTIL{Convergence.} \\STATE Set $P_k \\leftarrow L_kL_k^T$ for $k = 1,\\ldots,K$ \\STATE Set $\\ell \\leftarrow \\|(L_1^{-1},\\ldots,L_K^{-1})\\cdot R\\|$ \\STATE Set $W = (L_1^{-1},\\ldots,L_K^{-1})\\cdot R / \\ell$ \\RETURN $\\ell$, $W$, and $P_k$ for $k = 1,\\ldots,K$. \\end{algorithmic} \\end{algorithm} 3.5 Discussion In this chapter, we have presented a higher-order generalization of the LQ decomposition by reformulating the LQ decomposition as a minimization problem. The orthonormal rows property of the \\(Q\\) matrix in the LQ decomposition generalizes to the scaled all-orthonormal property of the mode-\\(k\\) unfoldings of the core array in the HOLQ. We generalized the HOLQ to HOLQ juniors by constraining the component matrices to subspaces of \\(\\mathcal{G}_{p_k}^+\\). One application of the HOLQ (junior) is for estimation and testing in separable covariance models. The MLEs of the covariance parameters may be recovered from the HOLQ (junior) and the likelihood ratio test has the simple form of the ratio of two scale estimates from the HOLQ junior. The core array from the HOLQ (junior) is ancillary with respect to the covariance parameters. We also used the HOLQ to develop a higher-order generalization of the SVD. Our version of the SVD can be viewed as a core rotation for the HOSVD (full rank case) or the HOOI (low rank case), where the core is rotated so that the mode specific singular values may be separated from the core array. We note that one can consider the model of Hoff (2013) as a model based truncated ISVD. He considered the model \\begin{align*} &amp;X \\sim N_{p_1\\times\\cdots\\times p_K}((U_1,\\ldots,U_K,I_n) \\cdot [(D_1,\\ldots,D_K,I_n) \\cdot V],\\sigma^2I_p), \\text{ where:}\\\\ &amp;U_k \\text{ is uniformly distributed on } \\mathcal{V}_{r_k,p_k},\\\\ &amp;D_k \\text{ has trace 1 and is uniformly distributed on the } r_k \\text{ simplex},\\\\ &amp;V \\sim N_{r_1\\times\\cdots\\times r_K}(0,\\tau^2I_r), \\text{ and}\\\\ &amp;\\tau^2 \\sim \\text{inverse-gamma}(1/2,\\tau_0^2/2), \\end{align*} where we changed the notation from his paper to make more clear the connection to the ISVD. In such a model, the core \\(V\\) is scaled all-orthonormal in expectation. That is, \\(E[V_{(k)}V_{(k)}^T] \\propto I_{p_k}\\) for all \\(k=1,\\ldots,K\\). One could extend his results by selecting a prior that allows for non-zero mass for the \\(D_k\\) to be of low rank, as in Hoff (2007) for his model based SVD. A clear limitation to the utility or the HOLQ or ISVD in practice is that in some dimensions they may not exist, and in other dimensions where they do exist, they may not be unique. The necessary and sufficient conditions for the existence and uniqueness of the HOLQ are not known. Sufficient conditions for existence and uniqueness occur when \\(n\\) is large. When \\(n \\geq p\\), the criterion function, \\(\\| (L_1^{-1},\\ldots,L_K^{-1},I_n) \\cdot X\\|\\), is bounded below by the value at the LQ decomposition. For \\(n\\) large enough, the HOLQ is also unique, this follows from the uniqueness of the MLE from Ohlson, Rauf Ahmad, and Rosen (2013). These conditions are equivalently sufficient for the existence and uniqueness of the ISVD. However, in the author’s experience, the HOLQ exists and is unique for many dimensions where \\(n &lt; p\\), indeed for many dimensions where \\(n = 1\\). In cases where the HOLQ/ISVD do not exist, the model of Hoff (2013) would be a good alternative. One could also construct a regularized version of the HOLQ. We note, however, that when a minimum is reached, then the HOLQ exists. This is due to the geodesic convexity results of the log-likelihood in Wiesel (2012a) and Wiesel (2012b). That is, any local minimum is also a global minimum. These results indicate that, for any particular data set, we can determine if any global minima exist. 7 References "],
["methods.html", "Chapter 4 Methods", " Chapter 4 Methods We describe our methods in this chapter. "],
["applications.html", "Chapter 5 Applications 5.1 Example one 5.2 Example two", " Chapter 5 Applications Some significant applications are demonstrated in this chapter. 5.1 Example one 5.2 Example two "],
["final-words.html", "Chapter 6 Final Words", " Chapter 6 Final Words We have finished a nice book. "],
["references.html", "Chapter 7 References", " Chapter 7 References "]
]
